# Helmchart RonDB

## Capabilities

- Create custom-size, cross-AZ cluster
- Horizontal auto-scaling of MySQLds & RDRS'

## Capabilities of Manual Intervention

- Scale data node replicas:
  1. Use the MGM client to activate Node IDs
  2. Increase `activeDataReplicas` in values.yaml. This will 
     1. Change the config.ini (important in case MGMds restart later)
     2. Increase the StatefulSet replicas.

## Backups

### Background info

- Backup Pod cannot be guaranteed access to volumes of data nodes (volumes can have ReadWriteOnce, depending on cloud)
- Native RonDB backups are performed on every replica of a node group
- Writing a backup directly into object storage is a bad idea; S3 has no append option; it would do unnecessarily many writes
- CSI storage drivers are neat, but more intransparent and thereby tricky to handle

### Backup structure

Backups are stored with the following file structure. `backup-id` is a 32-bit uint, used by RonDB natively.

```bash
<backup-id>
    users.sql  # MySQL user metadata
    databases.sql  # MySQL table metadata (including procedures & views)
    rondb  # Native RonDB data backup
        <datanode-node-id>
            BACKUP-<backup-id>-PART-1-OF-2/
            BACKUP-<backup-id>-PART-2-OF-2/
        <datanode-node-id>
            BACKUP-<backup-id>-PART-1-OF-2/
            BACKUP-<backup-id>-PART-2-OF-2/
        ...
```

The SQL files are generated by the MySQL servers whilst the native backups are created by the data nodes. The latter is triggered by the RonDB management client. The SQL files are not strictly necessary to restore the backup but can be helpful in the event of bugs. Also, the native backup does not contain MySQL views and procedures.

### Adding support for a new object storage

We use `rclone` to upload backups to object storage. `Rclone` is installed in the `hopsworks/hwutils` image and it claims to work for *a lot* of different storages, including OVH. To add support for a new object storage type, you need to add a configuration setting to rclone. For S3, this can look as follows:

```yaml
[myS3Remote]
type = s3
provider = AWS
access_key_id = blabla
secret_access_key = foofoo
```

An easy way of creating this file is to run `docker run --rm -it --entrypoint=/bin/bash rclone/rclone:latest` which will open a terminal in a rclone image. There you can run:

```bash
# This will open an interactive process where you can specify your object storage 
rclone config
# After this is done, run this to see where your config file is placed
rclone config file
```

### Restore backup steps

1. Start Data node Stateful Set:
   1. InitContainer: Download native backups on data nodes
   2. Main container: Run data nodes
2. Create Job A to:
   1. Wait for data nodes to start up
   2. Create *temporary* MySQLd that connects to the data nodes and
        creates system tables (important!). Otherwise, system tables
        will be restored by native backup.
   3. Run `ndb_restore --restore-meta --disable-indexes` on one ndbmtd
   4. Run `ndb_restore --restore-data` on all ndbmtds
   5. Run `ndb_restore --rebuild-indexes` on one ndbmtd
   6. Remove native backups on all ndbmtds
3. Create Job B to (*always* run this Job):
   1. (If available): Download MySQL metadata backup
   2. (If available): Wait for Job A
   3. Spawn *temporary* MySQLd that:
      1. (If available): Restores MySQL metadata backup
      2. Applies user-applied SQL init files
4. Start MySQLd Stateful Set:
   1. InitContainer: Initialize MySQLd data dir (no connection needed)
   2. Wait for Job B
   3. Main container: Run MySQLds with networking

## TODO

### Restore

- Remove all socket logic?
- Append "IF NOT EXISTS" to MySQL metadata when creating backups; probably best for single source of truth
- Split up procedures & views into separate SQL backup files
- Consider restoring procedures & views in separate Job at the end using LoadBalancer

Problem:
    Views & procedures are not replicated; hence they won't be restored properly by the Job. We would have to run the
    MySQL restore for every single Pod startup. It might be risky though to restore everything *over and over again*.
    One could consider separating procedures and views into a separate SQL file.

### Backups

- Remove DROP TABLE from backups (?)
- Create metadata.json file (up for discussion, it is not strictly necessary)
- Test with OVH object storage

### Other

- Create more values.yaml files for production settings
- Make data node memory changeable
- Add YCSB to benchmark options
- Figure out how to run `SELECT count(*) FROM ndbinfo.nodes` as MySQL readiness check.
  - Using  `--defaults-file=$RONDB_DATA_DIR/my.cnf` and `GRANT SELECT ON ndbinfo.nodes TO 'hopsworks'@'%';` does not work
  - Error: `ERROR 1356 (HY000): View 'ndbinfo.nodes' references invalid table(s) or column(s) or function(s) or definer/invoker of view lack rights to use them`

Kubernetes Jobs to add:
- Increasing logfile group sizes

## Quickstart

### Optional: Set up cloud object storage for backups

Periodical backups can be enabled using `--set backups.enabled`. These will be placed into cloud object storage.

Options:

1. **AWS S3 buckets**

    Create an S3 bucket and see this to have access to it:
    https://github.com/awslabs/mountpoint-s3-csi-driver/blob/main/docs/install.md#configure-access-to-s3

    Using Secrets is easiest, however the credentials need to be long-lived for that. An alternative is to try the following:
    https://chatgpt.com/share/3025f63f-9492-49e6-b46f-6f1ac37cde2f

### Run cluster

```bash
helm lint
helm template .

RONDB_NAMESPACE=rondb-default
kubectl create namespace $RONDB_NAMESPACE

# If periodical backups are enabled, add access to object storage.
# If using AWS S3:
kubectl create secret generic aws-credentials \
    --namespace=$RONDB_NAMESPACE \
    --from-literal "key_id=${AWS_ACCESS_KEY_ID}" \
    --from-literal "access_key=${AWS_SECRET_ACCESS_KEY}"

# Run this if both:
# - TLS Ingress/endToEnd is enabled in values
# - We are running standalone (without Hopsworks)
./setup_standalone_tls_dependencies.sh $RONDB_NAMESPACE

# Install and/or upgrade:
helm upgrade -i my-rondb \
    --namespace=$RONDB_NAMESPACE \
    --values ./values.minikube.small.yaml .
```

## Run tests

As soon as the Helmchart has been instantiated, we can run the following tests:

```bash
# Create some dummy data
helm test -n $RONDB_NAMESPACE my-rondb --logs --filter name=generate-data

# Check that data has been created correctly
helm test -n $RONDB_NAMESPACE my-rondb --logs --filter name=verify-data
```

***NOTE***: These Helm tests can also be used to verify that the backup/restore procedure was done correctly.

## Run benchmarks

Whilst the `size` in `minikube.<size>.yaml` determines the cluster power, it does not determine any cluster size. For benchmarks, minimal data node replication and many MySQL servers are best.

```bash
helm upgrade -i my-rondb \
    --namespace=$RONDB_NAMESPACE \
    --values ./values.minikube.small.yaml \
    --values ./values.benchmark.yaml .
```

## Teardown

```bash
helm delete --namespace=$RONDB_NAMESPACE my-rondb

# Remove other related resources (non-namespaced objects not removed here e.g. PriorityClass)
kubectl delete namespace $RONDB_NAMESPACE

# Remove cert-manager
kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.3/cert-manager.yaml

# Remove PVCs manually
kubectl delete pvc --all
```

## Minikube Values files

The `minikube.values.yaml` files are for single-machine configurations. Use other values for production settings.

- **mini**: 
  - Cluster setup: 1 MGM server, 1 data node, 1 MySQL server and 1 API node
  - Docker resource utilization: 2.5 GB of memory and up to 4 CPUs
  - Recommended machine: 8 GB of memory

- **small** (default):
  - Cluster setup: 1 MGM server, 2 data nodes, 2 MySQL servers and 1 API node
  - Docker resource utilization: 6 GB of memory and up to 16 CPUs
  - Recommended machine: 16 GB of memory and 16 CPUs

- **medium**:
  - Cluster setup: Same as **small**
  - Docker resource utilization: 16 GB of memory and up to 16 CPUs
  - Recommended machine: 32 GB of memory and 16 CPUs

- **large**:
  - Cluster setup: Same as **small**
  - Docker resource utilization: 20 GB of memory and up to 32 CPUs
  - Recommended machine: 32 GB of memory and 32 CPUs

- **xlarge**:
  - Cluster setup: Same as **small**
  - Docker resource utilization: 30 GB of memory and up to 50 CPUs
  - Recommended machine: 64 GB of memory and 64 CPUs

## Optimizations

Data nodes strongly profit from being able to lock CPUs. To be able to use a static CPU Manager Policy, one can start Minikube as follows:

```bash
minikube start \
    --driver=docker \
    --cpus=10 \
    --memory=21000MB \
    --feature-gates="CPUManager=true" \
    --extra-config=kubelet.cpu-manager-policy="static" \
    --extra-config=kubelet.cpu-manager-policy-options="full-pcpus-only=true" \
    --extra-config=kubelet.kube-reserved="cpu=500m"
```

## Sysbench Benchmarking


### Normal Setup

Using:
- Minikube on Mac M1 Pro: `minikube start --driver=docker --cpus 10 --memory 20000`
- For "power" of cluster: `values.minikube.small.yaml`

#### Cluster size: 1 MySQLd, 1 data node, max 370%

```bash
Threads: 1 Mean: 180
Threads: 2 Mean: 365
Threads: 4 Mean: 682
Threads: 8 Mean: 741
Threads: 12 Mean: 791
Threads: 16 Mean: 837
Threads: 24 Mean: 887
Threads: 32 Mean: 918
Threads: 64 Mean: 996
Threads: 128 Mean: 993
Threads: 256 Mean: 978
```

#### Cluster size: 2 MySQLds, 1 data node, max 660%

```bash
Threads: 1 Mean: 414
Threads: 2 Mean: 744
Threads: 4 Mean: 1160
Threads: 8 Mean: 1726
Threads: 12 Mean: 1870
Threads: 16 Mean: 1976
Threads: 24 Mean: 2165
Threads: 32 Mean: 2170
Threads: 64 Mean: 2244
Threads: 128 Mean: 2337
Threads: 256 Mean: 2245
```

#### 3 MySQLds, 1 data node, 1 bench, max 930%

```bash
Threads: 1 Mean: 595
Threads: 8 Mean: 2016
Threads: 16 Mean: 2505
Threads: 24 Mean: 2771
Threads: 32 Mean: 2694
Threads: 64 Mean: 3001
Threads: 128 Mean: 3033
Threads: 256 Mean: 2906
```

#### 4 MySQLds, 1 data node, 1 bench, max 980%

```bash
Threads: 1 Mean: 727
Threads: 2 Mean: 1150
Threads: 4 Mean: 1723
Threads: 8 Mean: 2237
Threads: 12 Mean: 2381
Threads: 16 Mean: 2650
Threads: 24 Mean: 2635
Threads: 32 Mean: 2623
Threads: 64 Mean: 2601
Threads: 128 Mean: 2723
Threads: 256 Mean: 2611
```

### Static CPU Setup

Using:
- Minikube on Mac M1 Pro with:
    ```bash
    minikube start \
        --driver=docker \
        --cpus=10 \
        --memory=21000MB \
        --feature-gates="CPUManager=true" \
        --extra-config=kubelet.cpu-manager-policy="static" \
        --extra-config=kubelet.cpu-manager-policy-options="full-pcpus-only=true" \
        --extra-config=kubelet.kube-reserved="cpu=500m"
    ```
- .Values.staticCpuManagerPolicy=true
- For "power" of cluster: `values.minikube.small.yaml`

### 2 MySQLds, 1 data node, 1 bench, max 970%

```bash
Threads: 1 Mean: 413
Threads: 2 Mean: 849
Threads: 4 Mean: 1637
Threads: 8 Mean: 2135
Threads: 12 Mean: 2407
Threads: 16 Mean: 2610
Threads: 24 Mean: 2804
Threads: 32 Mean: 2841
Threads: 64 Mean: 2987
Threads: 128 Mean: 3113
Threads: 256 Mean: 3236
```

### 3 MySQLds, 1 data node, 1 bench, max 980%

```bash
Threads: 1 Mean: 669
Threads: 2 Mean: 1285
Threads: 4 Mean: 2013
Threads: 8 Mean: 2480
Threads: 12 Mean: 2589
Threads: 16 Mean: 2657
Threads: 24 Mean: 2897
Threads: 32 Mean: 2798
Threads: 64 Mean: 2933
Threads: 128 Mean: 3085
Threads: 256 Mean: 3059
```
