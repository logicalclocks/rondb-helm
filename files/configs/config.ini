[NDBD DEFAULT] 

ServerPort=11860

DataDir=                    {{ include "rondb.ndbmtd.dataDir" $ }}
# PERSISTING IN-MEMORY DATA
# Local checkpoint data and REDO logs (cannot be separated)
FileSystemPath=             {{ include "rondb.ndbmtd.fileSystemPath" $ }}
# DISK COLUMN DATA (TABLESPACES)
# This overrides FileSystemPathDD (DD=disk data).
# This is where InitialTablespace is placed.
FileSystemPathDataFiles=    {{ include "rondb.ndbmtd.fileSystemPathDataFiles" $ }}
# UNDO LOGS / LOG FILE GROUPS
# This overrides FileSystemPathDD (DD=disk data).
# It is where the files for InitialLogFileGroup are placed.
FileSystemPathUndoFiles=    {{ include "rondb.ndbmtd.fileSystemPathUndoFiles" $ }}
BackupDataDir=              {{ include "rondb.ndbmtd.backupDataDir" $ }}

# RonDB default configuration is intended for execution in Cloud VMs with
# around 8 GB per VCPU. In this case it is sufficient to set nothing since
# AutomaticThreadConfig=true is default and also AutomaticMemoryConfig=true
# is default. The memory settings in this environment ensures that RonDB
# doesn't hit any memory boundaries in normal cases and at the same time
# providing large space for in-memory tables.
#
# In the Docker Compose environment we expect all nodes to execute in
# their own docker container on the same host. Thus we need to create a
# setting which works also in more limited memory environment.
#
# We set the TotalMemoryConfig to limit the amount of memory used in this
# environment. We set it to 2.5GB of memory which replicated becomes
# 5 GB of memory for data nodes.
# 
# In addition the other containers will use a bit more than 1 GB.
#
# The NumCPUs provides input to the AutomaticThreadConfig on how many
# threads to use. This will affect also memory usage, thus increasing
# this number requires also increasing the TotalMemoryConfig and
# potentially also other parameters will require changes.
#
# In a default setup the data node will run such that it uses all memory
# and all CPU available in the machine. This is the case since the default
# setup is aiming at a Cloud VM running only one data node. MySQL Servers
# and MGM servers are running in separate VMs as well.
#
# To run a benchmark on a high-end desktop with e.g. 32 CPUs one can
# set NumCPUs to 6 and increase TotalMemoryConfig a bit, for sysbench
# increasing to 4 GB of TotalMemoryConfig should be sufficient. For
# DBT2 each warehouse consumes an extra 100-200MB of memory. So running
# with e.g. 64 warehouses requires a machine with a bit more memory,
# it should work on a machine with 64 GB of memory.

# These are actually true by default anyways
AutomaticThreadConfig=true
AutomaticMemoryConfig=true

# Setting NumCPUs to 4 or some other number influences the number of
# threads of various types that we configure. In a development environment
# it is ok to set this larger than the number of actual CPUs (and smaller
# as well). In an environment where we own the entire VM it is desirable
# to not set NumCPUs such that the data node can figure out the appropriate
# number of threads according to the HW and even lock the threads to
# specific CPUs.
#
# These are exceptions to the automatic memory configuration.
# NumCPUs impacts both thread configuration and memory configuration. Thus
# raising NumCPUs means more memory is required in the data node.
{{ if not .Values.staticCpuManagerPolicy -}}
NumCPUs={{ printf "%.0f" (mulf .Values.resources.limits.cpus.ndbmtds 0.9) }}
{{- end }}

{{ $totalMemoryConfig := sub .Values.resources.limits.memory.ndbmtdsMiB  .Values.rondbConfig.NdbmtdMemoryOverheadMiB }}
{{- if le $totalMemoryConfig 0 }}
{{- fail "TotalMemoryConfig cannot be negative number. Set the NDBMTD container memory limit higher than the NdbmtdMemoryOverheadMiB overhead." }}
{{- end }}

TotalMemoryConfig={{ $totalMemoryConfig }}M

{{ if .Values.rondbConfig.DiskPageBufferMemory -}}
DiskPageBufferMemory={{ .Values.rondbConfig.DiskPageBufferMemory }}
{{- end }}

{{ if .Values.rondbConfig.MaxNoOfTables -}}
MaxNoOfTables={{ .Values.rondbConfig.MaxNoOfTables }}
{{- end }}

{{ if .Values.rondbConfig.MaxNoOfAttributes -}}
MaxNoOfAttributes={{ .Values.rondbConfig.MaxNoOfAttributes }}
{{- end }}

{{ if .Values.rondbConfig.MaxNoOfTriggers -}}
MaxNoOfTriggers={{ .Values.rondbConfig.MaxNoOfTriggers }}
{{- end }}

{{ if .Values.rondbConfig.TransactionMemory -}}
TransactionMemory={{ .Values.rondbConfig.TransactionMemory }}
{{- end }}

{{ if .Values.rondbConfig.SharedGlobalMemory -}}
SharedGlobalMemory={{ .Values.rondbConfig.SharedGlobalMemory }}
{{- end }}

{{ if .Values.rondbConfig.ReservedConcurrentOperations -}}
ReservedConcurrentOperations={{ .Values.rondbConfig.ReservedConcurrentOperations }}
{{- end }}

RedoBuffer=16M

{{ if .Values.rondbConfig.ReplicationMemory -}}
ReplicationMemory={{ .Values.rondbConfig.ReplicationMemory }}
{{- end }}

{{ if .Values.rondbConfig.SchemaMemory -}}
SchemaMemory={{ .Values.rondbConfig.SchemaMemory }}
{{- end }}

# Size of the REDO log is NoOfFragmentLogParts * NoOfFragmentLogFiles * FragmentLogFileSize
# A smaller REDO log means one will have to be more active in checkpointing and at really high loads 
# one needs a larger REDO log, but in development 2 GiB should be quite sufficient. In production
# 64 GiB is more or less always enough. Further constraints:
# - The maximum FragmentLogFileSize is 1GiB
# - 4 NoOfFragmentLogParts is standard
{{- $redoLogSizeGiB := ($.Values.resources.requests.storage.redoLogGiB | int) }}
{{- $redoLogSizeMiB := mul $redoLogSizeGiB 1024 }}
{{- /*
    The size of the redo log is calculated as follows:
    redoLogSize = noOfFragmentLogParts * noOfFragmentLogFiles * fragmentLogFileSize
    fragmentLogFileSize = redoLogSize / (noOfFragmentLogParts * noOfFragmentLogFiles)
    If fragmentLogFileSize > 1024 MiB, then set fragmentLogFileSize to 1024 MiB and recalculate noOfFragmentLogFiles
*/}}
{{- $noOfFragmentLogParts := 4 }}
{{- $noOfFragmentLogFiles := 4 }}
{{- $sizePerPartMiB := div $redoLogSizeMiB $noOfFragmentLogParts }}
{{- $fragmentLogFileSizeMiB := div $sizePerPartMiB $noOfFragmentLogFiles }}
{{- if gt $fragmentLogFileSizeMiB 1024 }}
    {{- $fragmentLogFileSizeMiB = 1024 }}
    {{- $noOfFragmentLogFiles = div $sizePerPartMiB $fragmentLogFileSizeMiB -}}
{{- end }}
NoOfFragmentLogParts={{ $noOfFragmentLogParts }}
NoOfFragmentLogFiles={{ $noOfFragmentLogFiles }}
FragmentLogFileSize={{ $fragmentLogFileSizeMiB }}M

NoOfReplicas=3

# Setting this parameter to TRUE or 1 binds IP_ADDR_ANY so that connections can be made from anywhere (for autogenerated connections). The default is FALSE (0).
TcpBind_INADDR_ANY=FALSE

{{ if .Values.rondbConfig.MaxNoOfConcurrentOperations -}}
MaxNoOfConcurrentOperations={{ .Values.rondbConfig.MaxNoOfConcurrentOperations }}
{{- end }}

MaxDMLOperationsPerTransaction=32K

MaxNoOfConcurrentScans=500

#The maximum time in ms that is permitted to lapse between operations in the same transaction before the transaction is aborted.
TransactionInactiveTimeout={{ .Values.rondbConfig.TransactionInactiveTimeout | default 15000 }}
TransactionDeadlockDetectionTimeout={{ .Values.rondbConfig.TransactionDeadlockDetectionTimeout | default 1500 }}

#0: Disables locking. This is the default value.
#1: Performs the lock after allocating memory for the process.
#2: Performs the lock before memory for the process is allocated.
# Needs root privileges
LockPagesInMainMemory=0

#SpinMethod in cloud setups the default gives better latency.
#In shared environments like this it is better to avoid spinnning.
{{ if not .Values.staticCpuManagerPolicy -}}
SpinMethod=StaticSpinning
{{- end }}

# Savings of up to 50% over noncompressed LCPs and backups
CompressedLCP=0
CompressedBackup=1

BackupLogBufferSize= 16M

# The maximum size of the memory unit to use when allocating memory for tables
MaxAllocate=32M

# Needs root privileges
ODirect=0

Numa=1

# Move this to another drive if you have a high number of ops/sec
InitialLogFileGroup=name=lg_1;undo_buffer_size=128M;undo_log_0.log:{{ $.Values.resources.requests.storage.undoLogsGiB }}G

{{- $initialTS := .Values.rondbConfig.InitialTablespaceSizeGiB | int }}
{{- if le $initialTS 0 }}
{{- $initialTS = .Values.resources.requests.storage.diskColumnGiB }}
{{- else if gt $initialTS (int .Values.resources.requests.storage.diskColumnGiB )}}
{{- fail "InitialTablespaceSizeGiB cannot be greater than diskColumnGiB." }}
{{- end }}

InitialTablespace=name=ts_1;extent_size=16M;ts_1_data_file_0.dat:{{ $initialTS }}G

[MYSQLD DEFAULT]

[NDB_MGMD DEFAULT]
# This is where the configuration database is stored
DataDir={{ include "rondb.dataDir" $ }}/mgmd
LogDestination=FILE:filename={{ include "rondb.dataDir" $ }}/log/cluster.log,maxsize=10000000,maxfiles=6

[TCP DEFAULT]
OverloadLimit=0

# Without this flag, ndbds could not start up if not all containers
# listed in the config.ini were running. E.g. the ndbds would try to 
# to resolve the hostname "bench_1", whereby the container "bench_1" has
# been stopped. The ndbd would not find anything, and then fail.
AllowUnresolvedHostnames=true

# Inactive Data nodes, to enable increase of replication level
# Hostname doesn't matter since it isn't allowed to connect to
# the cluster and before this can be done the node must be
# activated and an inactive node can change its hostname.
# Inactive nodes have NodeActive set to 0.

{{ range $nodeGroup := until ($.Values.clusterSize.numNodeGroups | int) -}}
{{ range $replica := until 3 -}}
{{ $isActive := 0 -}}
{{ if lt $replica ($.Values.clusterSize.activeDataReplicas | int) -}}
  {{ $isActive = 1 -}}
{{ end -}}
{{ $offset := ( mul $nodeGroup 3) -}}
{{ $nodeId := ( add $offset (add $replica 1)) -}}
{{ $hostname := (printf "node-group-%d-%d.ndbmtd-ng-%d.%s.svc.cluster.local" 
    $nodeGroup
    $replica
    $nodeGroup
    $.Release.Namespace
) -}}
{{ include "config_ndbd" (dict
    "nodeId" $nodeId
    "nodeGroup" $nodeGroup
    "isActive" $isActive
    "hostname" $hostname
)}}

{{ end }}
{{- end }}

[NDB_MGMD]
NodeId=65
LocationDomainId=0
HostName={{ include "rondb.mgmdHostname" . }}
PortNumber=1186
NodeActive=1
ArbitrationRank=2

{{ $mysqldStartNodeId := 67 -}}
{{/*
    Each server can have multiple node IDs to establish multiple
    cluster connections.
*/}}
{{ range $nthServer := until ($.Values.clusterSize.maxNumMySQLServers | int) -}}
{{ $serverOffset := ( add $mysqldStartNodeId ( mul $nthServer $.Values.rondbConfig.MySQLdSlotsPerNode)) -}}
{{ range $nthConnection := until ($.Values.rondbConfig.MySQLdSlotsPerNode | int) -}}
{{ $mysqldNodeId := add $serverOffset (add $nthConnection) -}}
{{ $hostname := (printf "%s-%d.%s.%s.svc.cluster.local"
    $.Values.meta.mysqld.statefulSet.name
    $nthServer
    $.Values.meta.mysqld.headlessClusterIp.name
    $.Release.Namespace
) -}}
{{ include "config_mysqld" (dict "nodeId" $mysqldNodeId "isActive" 1 "hostname" $hostname) }}

{{ end }}
{{ end -}}

####################
## Binlog servers ##
####################

{{ $binlogStartNodeId := add
    $mysqldStartNodeId
    (mul
        $.Values.clusterSize.maxNumMySQLServers
        $.Values.rondbConfig.MySQLdSlotsPerNode
    )
-}}
{{ range $nthServer := until ($.Values.globalReplication.primary.maxNumBinlogServers | int) -}}
{{ $binlogNodeId := add $binlogStartNodeId $nthServer -}}
{{ $hostname := (printf "%s-%d.%s.%s.svc.cluster.local"
    $.Values.meta.binlogServers.statefulSet.name
    $nthServer
    $.Values.meta.binlogServers.headlessClusterIp.name
    $.Release.Namespace
) -}}
{{ include "config_mysqld" (dict "nodeId" $binlogNodeId "isActive" 1 "hostname" $hostname) }}

{{ end -}}

######################
## Replica appliers ##
######################

{{ $replicaApplierStartNodeId := add
    $binlogStartNodeId
    ($.Values.globalReplication.primary.maxNumBinlogServers | int)
-}}
{{ $hostname := (printf "%s-%d.%s.%s.svc.cluster.local"
    $.Values.meta.replicaAppliers.statefulSet.name
    0
    $.Values.meta.replicaAppliers.headlessClusterIp.name
    $.Release.Namespace
) -}}
{{- include "config_mysqld" (dict "nodeId" $replicaApplierStartNodeId "isActive" 1 "hostname" $hostname) }}

##################
## RDRS servers ##
##################

# We will use x slots per data cluster and y slots per metadata cluster.
# If we have an external metadata cluster, we can ignore the metadata
# cluster slots here.

{{- $rdrsSlotsPerNode := $.Values.rondbConfig.RdrsSlotsPerNode }}
{{- if eq (len $.Values.rdrs.externalMetadataCluster.mgmds) 0 }}
{{- $rdrsSlotsPerNode = (add $rdrsSlotsPerNode $.Values.rondbConfig.RdrsMetadataSlotsPerNode) | int }}
{{- end }}
{{ range $nthServer := until ($.Values.clusterSize.maxNumRdrs | int) -}}
{{ range $nthConnection := until ($rdrsSlotsPerNode | int) -}}
{{ $offset := ( add 195 ( mul $nthServer $rdrsSlotsPerNode)) -}}
{{ $nodeId := add $offset (add $nthConnection) -}}
{{ include "config_api" (dict 
    "nodeId" $nodeId
    "isActive" 1
    "hostname" (printf "%s-%d.%s.%s.svc.cluster.local" 
        $.Values.meta.rdrs.statefulSet.name
        $nthServer
        $.Values.meta.rdrs.headlessClusterIpName
        $.Release.Namespace
    )
)}}

{{ end }}
{{- end }}

###########################
## Empty API slots       ##
## e.g. Clusterj clients ##
###########################

{{ $totalRdrsSlots := ( mul $.Values.clusterSize.maxNumRdrs $rdrsSlotsPerNode) -}}
{{ $baseOffset := ( add 195 $totalRdrsSlots ) -}}
{{ range $nthSlot := until ($.Values.rondbConfig.EmptyApiSlots | int) -}}
{{ $nodeId := add $baseOffset (add $nthSlot) -}}
{{ include "config_api" (dict 
    "nodeId" $nodeId
    "isActive" 1
    "hostname" ""
)}}

{{ end }}
