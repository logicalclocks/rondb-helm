apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: ndb-high-priority
  namespace: {{ .Release.Namespace }}
value: 1000000
globalDefault: false
description: "This priority class should be used for rondb service pods only."
---
{{- range $nodeGroup := until ($.Values.clusterSize.numNodeGroups | int) }}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  # This will have the form of "node-group-0-0", "node-group-0-1", etc.
  name: node-group-{{ $nodeGroup }}
  namespace: {{ $.Release.Namespace }}
spec:
  serviceName: ndbmtd-ng-{{ $nodeGroup }}
  # When using a startup probe, one ndbmtd is only started after
  # the other is fully connected. However, when starting up a
  # cluster (using flag --initial), the MGMd will wait for all
  # data nodes to start at the same time.
  podManagementPolicy: Parallel
  replicas: {{ $.Values.clusterSize.activeDataReplicas }}
{{ if lt ($.Values.clusterSize.activeDataReplicas | int) 2 }}
  # Don't kill a data node if we only have 1 replica.
  # Even when we start with 2 replicas and scale down to 1,
  # whilst also scaling our replicas vertically, K8s
  # will first kill the second replica before exchanging our
  # first replica. In short: even if we start with
  # 2 replicas, we risk downtime when vertically
  # scaling.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 1
{{ end }}
  selector:
    # Used by the Deployment to select and manage existing pods with the specified label
    matchLabels:
      rondbService: ndbmtd
      nodeGroup: {{ $nodeGroup | quote }}
  # StatefulSets work with PVCs to create a dedicated persistent volume for
  # each pod replica, ensuring that a pod always re-attaches to the same data
  # even if it is rescheduled to a different node.
  volumeClaimTemplates:
    - metadata:
        name: rondb-ndbmtd
      spec:
        accessModes: [ReadWriteOnce]
{{ include "rondb.storageClassName" $ | indent 8 }}
        resources:
          requests:
{{- $backupGiB := add
    (div $.Values.resources.limits.memory.ndbmtdsMiB 1024)
    $.Values.resources.requests.storage.undoLogsGiB
}}
{{- if $.Values.resources.requests.storage.dedicatedDiskColumnVolume.enabled }}
            storage: {{ add
                $backupGiB
                $.Values.resources.requests.storage.redoLogGiB
                $.Values.resources.requests.storage.undoLogsGiB
                $.Values.resources.requests.storage.slackGiB
                $.Values.resources.requests.storage.logGiB
            }}Gi
    - metadata:
        name: rondb-ndbmtd-dc
      spec:
        accessModes: [ReadWriteOnce]
{{ include "rondb.diskColumn.storageClassName" $ | indent 8 }}
        resources:
          requests:
            storage: {{ add
                $.Values.resources.requests.storage.diskColumnGiB 
                $.Values.resources.requests.storage.slackGiB
            }}Gi
{{- else }}
            storage: {{ add
                $backupGiB
                $.Values.resources.requests.storage.diskColumnGiB
                $.Values.resources.requests.storage.redoLogGiB
                $.Values.resources.requests.storage.undoLogsGiB
                $.Values.resources.requests.storage.slackGiB
                $.Values.resources.requests.storage.logGiB
            }}Gi
{{- end }}
  # Still in beta mode (Jan 2024)
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Retain
  template:
    metadata:
      # Used to apply labels to all pods created by the StatefulSet
      labels:
        rondbService: ndbmtd
        nodeGroup: {{ $nodeGroup | quote }}
    spec:
{{- include "hopsworkslib.imagePullSecrets" $ | indent 6 }}
{{- include "rondb.SecurityContext" $ | indent 6 }}
      affinity:
        podAntiAffinity:
{{- if not $.Values.isMultiNodeCluster }}
          # Must be on different nodes
          requiredDuringSchedulingIgnoredDuringExecution:
          # Replicas of same node group must be on different nodes
          - labelSelector:
              matchExpressions:
                - key: nodeGroup
                  operator: In
                  values:
                  - {{ $nodeGroup | quote }}
            topologyKey: kubernetes.io/hostname
          # Data nodes must be on different nodes than MGMds
          # Example, we have:
          # - a replication factor of 2
          # - 1 node group for simplicity
          # - one replica on the same node as the MGMd and this node goes down
          # Then the data node on the other node would not be able to
          # use the MGMd for arbitration and would then also go down.
          - labelSelector:
              matchExpressions:
                - key: rondbService
                  operator: In
                  values:
                  - mgmd
            topologyKey: kubernetes.io/hostname
{{- end }}
          # Can be in different zones
          preferredDuringSchedulingIgnoredDuringExecution:
          # Placing a MGMd into a different AZ will not affect performance at all,
          # but greatly benefit the availability of the cluster.
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: rondbService
                  operator: In
                  values:
                  - mgmd
              topologyKey: topology.kubernetes.io/zone
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: nodeGroup
                  operator: In
                  values:
                  - {{ $nodeGroup | quote }}
              topologyKey: topology.kubernetes.io/zone
      priorityClassName: "ndb-high-priority"
      terminationGracePeriodSeconds: {{ $.Values.terminationGracePeriodSeconds }}
      initContainers:
      - name: mgmd-dependency-check
        image: {{ include "image_address" (dict "image" $.Values.images.rondb) }}
        # Setting resources in order to be in QoS class Guaranteed
        resources:
          limits:
{{- if $.Values.staticCpuManagerPolicy }}
            # cpu *has to* be set to 1, otherwise the QoS class will be Burstable
            cpu: 1
{{- else }}
            cpu: 0.2
{{- end }}
            memory: 200Mi
          requests:
            memory: 200Mi
        env:
        - name: MGMD_HOST
          value: {{ $.Values.meta.mgmd.statefulSetName }}-0.{{ $.Values.meta.mgmd.headlessClusterIp.name }}.{{ $.Release.Namespace }}.svc.cluster.local
        - name: NODE_GROUP
          value: {{ $nodeGroup | quote }}
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        command:
        - /bin/bash
        - -c
        - |
          until nslookup $MGMD_HOST; do
            echo "Waiting for $MGMD_HOST to be resolvable..."
            sleep $(((RANDOM % 2)+2))
          done

{{ include "rondb.nodeId" $ | indent 10 }}

          # Activating node slots is idempotent anyways
          # Activating can take some seconds anyways, hence the code afterwards is helpful
          set -e
          echo "Activating node id $NODE_ID via MGM client"
          ndb_mgm --ndb-connectstring=$MGMD_HOST:1186 -e "$NODE_ID activate"
          set +e

          echo "Making sure the pod hostname resolves to the correct IP address before connecting to the MGMd"
          POD_IP=$(hostname -i)
          echo "POD_IP: $POD_IP"
          POD_DNS_NAME="node-group-{{ $nodeGroup }}-$POD_ID.ndbmtd-ng-{{ $nodeGroup }}.{{ $.Release.Namespace }}.svc.cluster.local"

          while true; do
              # Use nslookup to resolve the DNS name to an IP
              nslookup $POD_DNS_NAME
              RESOLVED_IP=$(nslookup $POD_DNS_NAME | awk '/^Address: / { print $2 }' | head -n 1)

              if [ "$RESOLVED_IP" = "$POD_IP" ]; then
                  echo "The pod's resolved hostname and its IP address match."
                  exit 0;
              else
                  echo "Mismatch in IP addresses. DNS resolution incorrect."
                  sleep 1
              fi
          done
      containers:
      - name: ndbmtd
        image: {{ include "image_address" (dict "image" $.Values.images.rondb) }}
        command:
        - /bin/bash
        - -c
        - |
{{ tpl ($.Files.Get "files/entrypoints/ndbmtds.sh") $ | indent 10 }}
        ports:
          - containerPort: 11860
        resources:
          # If not specified, requested resources are set to the limits.
          # This is important to acquire the QoS class Guaranteed.
          limits:
            cpu: {{ $.Values.resources.limits.cpus.ndbmtds }}
            memory: {{ $.Values.resources.limits.memory.ndbmtdsMiB }}Mi
        env:
          - name: MGM_CONNECTION_STRING
            value: {{ $.Values.meta.mgmd.statefulSetName }}-0.{{ $.Values.meta.mgmd.headlessClusterIp.name }}.{{ $.Release.Namespace }}.svc.cluster.local:1186
          - name: MGMD_HOST
            value: {{ $.Values.meta.mgmd.statefulSetName }}-0.{{ $.Values.meta.mgmd.headlessClusterIp.name }}.{{ $.Release.Namespace }}.svc.cluster.local
          - name: NODE_GROUP
            value: {{ $nodeGroup | quote }}
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: FILE_SYSTEM_PATH
            value: /srv/hops/mysql-cluster/ndb_data
        startupProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 15
          failureThreshold: 150
          periodSeconds: 10
          timeoutSeconds: 10
        # This has to account for a failing MGMd as well.
        # There is no point in killing the data node
        # if the MGMd is not up / ready.
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set +e
              nslookup $MGMD_HOST > /dev/null 2>&1
              NS_LOOKUP_EXIT_CODE=$!
              if [ $NS_LOOKUP_EXIT_CODE -ne 0 ]; then
                  echo "MGMd at '$MGMD_HOST' is not ready, we cannot check whether the data node is connected"
                  exit 0
              fi
              set -e
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 5
          failureThreshold: 10
          periodSeconds: 10
          timeoutSeconds: 10
        # Adding this reduces the likelihood of continuing with a
        # rolling update when something is off. We might not want
        # to restart the data node just yet, but we want to flash
        # some warning signals. API nodes will still be able to connect
        # if the data nodes are not ready since publishNotReadyAddresses
        # is set to True.
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 5
          failureThreshold: 3
          periodSeconds: 5
          timeoutSeconds: 5
        volumeMounts:
        - name: rondb-ndbmtd
          mountPath: /srv/hops/mysql-cluster/rondb
{{- if $.Values.resources.requests.storage.dedicatedDiskColumnVolume.enabled }}
        - name: rondb-ndbmtd-dc
          mountPath: /srv/hops/mysql-cluster/ndb_data_files
{{- end }}
{{- if $.Values.backups.enabled -}}
{{/* This container is used to receive calls to push backups to object storage */}}
      - name: rclone-listener
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        command:
        - /bin/bash
        - -c
        - |
          set -e
{{ include "rondb.createRcloneConfig" . | indent 10 }}
          sleep infinity
        resources:
          # If not specified, requested resources are set to the limits.
          # This is important to acquire the QoS class Guaranteed.
          limits:
{{- if $.Values.staticCpuManagerPolicy }}
            # cpu *has to* be set to 1, otherwise the QoS class will be Burstable
            cpu: 1
{{- else }}
            cpu: 0.3
{{- end }}
            memory: 300Mi
        env:
{{- if eq $.Values.backups.objectStorageProvider "s3" }}
        - name: ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.backups.s3.keyCredentialsSecret | nindent 14 }}
        - name: SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.backups.s3.secretCredentialsSecret | nindent 14 }}
{{- end }}
        - name: RCLONE_MOUNT_FILEPATH
          value: &rawRCloneConf /home/hopsworks/rclone-raw.conf
        # This will be read by rclone
        - name: RCLONE_CONFIG
          value: /home/hopsworks/rclone.conf
        volumeMounts:
        - name: rondb-ndbmtd
          mountPath: /home/hopsworks/data
        - name: backup-configs
          mountPath: *rawRCloneConf
          subPath: rclone.conf
      volumes:
      - name: backup-configs
        configMap:
          name: backup-configs
{{- end }}
---
apiVersion: v1
kind: Service
metadata:
  # Match the spec.serviceName
  name: ndbmtd-ng-{{ $nodeGroup }}
  namespace: {{ $.Release.Namespace }}
spec:
  # Headless service for individual DNS records for the pods
  clusterIP: None
  # So we do not rely on the readiness probe to connect to the MGMd
  publishNotReadyAddresses: true
  # Match the spec.template.metadata.labels of the StatefulSet
  selector:
    rondbService: ndbmtd
    nodeGroup: {{ $nodeGroup | quote }}
  ports:
    - protocol: TCP
      port: 11860
      targetPort: 11860
---
{{ end }}
